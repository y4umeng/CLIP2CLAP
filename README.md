# CLIP2CLAP
Embedding space alignment with attention

Abstract: Image and audio files are large, usually many megabytes each. With low computational resources on hand, training a neural network with either of these media types as input data is a difficult task. Faced with this dilemma, one might turn to a pretrained model that can already solve the task at hand. This method only works if such a model exists. For an uncommon task like image-conditioned audio generation, this likely won’t be the case. Instead, you may have to use multiple models in sequence. Embedding space alignment is one method to tie models together. With multimodal latent spaces, such as OpenAI’s CLIP, aligning two latent spaces along one mode, such as text, has the side effect of aligning every other mode pair between the two spaces. This has been proven to be effective by Meta’s ImageBind. In their paper they showed that a joint embedding space over multiple modes (images, text, audio, depth, thermal, and IMU data) could be trained on data of each mode paired solely with images. In this case I’ll be using text as the “binding” data between images and audio. I’ll be taking two pretrained models, OpenAI’s CLIP, an image/text joint encoder, and LAION’s CLAP, an audio/text joint encoder, and aligning them using only text data. Then, with a pretrained version of Google’s audio generation model AudioLM that takes CLAP audio embeddings as conditioning, I will be able to do image-conditioned audio generation.  
