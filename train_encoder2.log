Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]Loading pipeline components...:  17%|█▋        | 1/6 [00:00<00:00,  7.57it/s]Loading pipeline components...:  33%|███▎      | 2/6 [00:00<00:00,  7.34it/s]Loading pipeline components...:  83%|████████▎ | 5/6 [00:00<00:00, 12.90it/s]Loading pipeline components...: 100%|██████████| 6/6 [00:00<00:00, 12.71it/s]
Length of test data: 25014
Length of train data: 591753
Number of parameters: 474013
Adjusting learning rate of group 0 to 3.0000e-04.
/home/njd2135/yw3809/.venv/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Loss at epoch 0, batch 0: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 500: tensor([0.0354, 0.0352], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 1000: tensor([0.0353, 0.0352], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 1500: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 2000: tensor([0.0354, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 2500: tensor([0.0353, 0.0354], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 3000: tensor([0.0352, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 3500: tensor([0.0353, 0.0354], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 4000: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 4500: tensor([0.0355, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 5000: tensor([0.0352, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 5500: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 6000: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 6500: tensor([0.0353, 0.0354], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 7000: tensor([0.0352, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 7500: tensor([0.0354, 0.0351], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 8000: tensor([0.0355, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 8500: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 0, batch 9000: tensor([0.0353, 0.0352], device='cuda:0', grad_fn=<GatherBackward>)
Adjusting learning rate of group 0 to 2.7000e-04.
Loss at epoch 1, batch 0: tensor([0.0352, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 500: tensor([0.0354, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 1000: tensor([0.0352, 0.0354], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 1500: tensor([0.0352, 0.0352], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 2000: tensor([0.0354, 0.0354], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 2500: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 3000: tensor([0.0352, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 3500: tensor([0.0352, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 4000: tensor([0.0354, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 4500: tensor([0.0353, 0.0352], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 5000: tensor([0.0353, 0.0352], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 5500: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 6000: tensor([0.0352, 0.0352], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 6500: tensor([0.0354, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 7000: tensor([0.0353, 0.0352], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 7500: tensor([0.0354, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 8000: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 8500: tensor([0.0354, 0.0352], device='cuda:0', grad_fn=<GatherBackward>)
Loss at epoch 1, batch 9000: tensor([0.0353, 0.0353], device='cuda:0', grad_fn=<GatherBackward>)
Adjusting learning rate of group 0 to 2.4300e-04.
Loss at epoch 2, batch 0: tensor([0.0352, 0.0354], device='cuda:0', grad_fn=<GatherBackward>)
